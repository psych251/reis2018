---
title: "Replication of Study 5 by Reis et al. (2018, Journal of Experimental Social Psychology)"
author: "Catherine Garton (cgarton@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

### Background
Reis and colleagues (2018) reported a series of experiments that examine the interpersonal influences on intellectual humility (IH)^[IH can be thought of as the awareness of one's epistemic limitations, i.e., to what extent you understand that your beliefs might be wrong or may change in the future.]. The authors hypothesized that priming the social cue of partner responsiveness would boost IH, because feeling understood may decrease people's propensity for self-enhancement and allow them to respond to their environment more openly. Study 5, which I will attempt to replicate, finds that priming partner responsiveness promotes a broader cognitive perspective (as measured in a global-local visual orientation task). The authors imply that a broader perspective may be the cognitive underpinnings of intellectual humility.

### Justification
Intellectual humility has been the backbone of my research interests. The affective roots of IH are incredibly understudied, and I am hoping to explore the roles of threat and competition (vs. psychological safety and trust) in enabling intellectual humility about morally divisive or identity-based beliefs. This is the most relevant paper in the extant literature, because it provides preliminary evidence that feelings of safety and support increase cognitive openness.

### Stimuli and procedures
Participants will be recruited from Prolific and randomized to one of two conditions^[Note that this differs from the original study, in which there was also a neutral control. However, I am underpowered to examine three conditions, and the original effect was only found between the responsive and unresponsive conditions.]: responsive or unresponsive partner. In the responsive (vs. unresponsive) condition, participants are asked to describe 2 (vs. 10) kind or considerate things their partner had done for them in the past week. It is generally easy to think of 2 such examples and harder to think of 10; this relative availability primes confidence or doubt in one's partner's responsiveness. Then participants will complete 16 trials of the global-local visual orientation task (see example image below). Finally, participants will complete the 20-item PANAS measure of mood, a potential confounding variable. I expect the study to be programmable in Qualtrics and to take no longer than 8 minutes per participant.

<center>
![Fig. 1, Reis et al. (2018), p. 30](/Users/catherinegarton/Documents/Grad School/Class and admin/251/reis2018/images/global_local.png){width=50%}
</center>


### Anticipated challenges
* Appropriate power. The authors recruited 225 participants per condition, for a target of 188 after deletions, based on expected effect sizes. In a post-hoc power analysis (based on the magnitude of the effect the authors found, assuming no bias in the estimate), their sample size afforded 99% power. Given the budget for this class, I proposed a target sample of 180 participants after exclusions^[I will account for a 15% buffer for exlusions] (90 per condition), which would provide 80% power for an effect size of the same magnitude, **d** = .37.

### Links
View the project repository on Github: [Psych 251 repo](https://github.com/psych251/reis2018){target="_blank"}

View the original paper: [Reis et al., 2018](https://github.com/psych251/reis2018/blob/main/original_paper/Reis_et_al_2018.pdf){target="_blank"}

View the replication survey: [Qualtrics](https://stanforduniversity.qualtrics.com/jfe/form/SV_783wI0qv7YyaeXA)

***

## Methods

### Power Analysis

I used G*power to conduct a power analysis for a between-subjects, unpooled, one-tailed t-test. Given the original effect size, I would need 180 participants to have 80% power to detect the effect at alpha = .05. I expect a significant number of participants to fail the attention check, so with 15% buffer for exclusions, I aim to recruit 212 participants.

### Planned Sample

Using Prolific, I will recruit English-speaking adults (*n* = 212). I will terminate data collection when I have reached this predetermined sample size, as indicated by my power analysis. There are no other pre-selection rules.

### Materials

Participants will complete a survey hosted by Qualtrics. They will view and select geometric images as part of a 16-trial global-local visual orientation task originally developed by Kimchi & Palmer (1982). Survey materials and images can be found on the [Harvard Dataverse](https://dataverse.harvard.edu/privateurl.xhtml?token=f8d2d590-6a8e-4df2-9dca-7e3f3813c78c). 

### Procedure	

To mimic the original study exactly, the study title will advertised as “relationships and perception”. Individuals who enroll in the study on Prolific will be redirected to a Qualtrics survey, where they will provide informed consent before continuing:

**1. Manipulation**:
Participants will first complete a writing prompt based on one of two randomized conditions. I will follow the original authors' procedure for the responsive (unresponsive) conditions by asking, "Please describe **2 (10) kind or considerate** things your partner has done to help you **in the last 7 days.**" Participants are then given 2 (10) short-answer text boxes to fill in their answers. Note that participants are instructed to complete the question about a close friend if they are not currently in a romantic relationship. 

**2. Global-local visual orientation task** (following Reis & colleagues' protocol exactly):

> On each trial, participants were shown a screen with a geometric shape (composed of triangles or squares) on top and beneath this single shape, two images containing multiple shapes. The participant is asked to judge whether the single shape is more similar to the lower-right or lower-left image. . . . Across trials, the right-left position of the global-local option was counterbalanced, as was the shape associated with each option. 
> (Reis et al., 2019, p. 29)

**3. Mood**:
After this task, participants will complete the 20-item PANAS (Watson, Clark, & Tellegen, 1988) to assess their mood.

**4. Attention check & demographics**:
Finally, participants will respond to a manipulation check (i.e., the ease or difficulty of the relationship recall task), an attention check, and a demographic question about their sex.


### Analysis Plan

#### Data cleaning 
Data will be excluded from analysis if any of the following criteria apply (according to reasons cited by the original authors):

* failing the attention check
* leaving the relationship recall task items blank
* skipping more than half of the global-local visual orientation trials
* choosing the same response on all 16 trials^[I don't understand the logic of this exclusion, but I am retaining it for consistency with the original paper]

#### Data processing
The independent variable, perceived responsiveness, will be captured by Qualtrics embedded data according to participants' randomized condition.

The dependent variable, cognitive broadening, will be calculated by summing the number of trials of which participants selected the "global processing" image (i.e., the image with pattern-based similarity rather than constituent-shape similarity). 
Note: Based on a reproducibility analysis I completed with the authors' public data, I determined that the authors first summed global processing scores within the first 8 trials (which contain small figures containing 3-4 blocks each) and within the last 8 trials (which contain large figures containing 9-10 blocks each). Within each of these sets, they interpolated any NA values by calculating the mean likelihood of a global-processing choice for the completed trials and substituting that value for the missing trial(s). They then summed the small-figure trials and large-figure trials to create a composite global processing score, which was used for the ANOVA. I will follow the authors' analytic choices to preserve the consistency of the replication.

A current mood score will be created by subtracting the mean score across negative items from the mean score across positive items. 

### Key analysis
The key analysis is a between-subjects unpaired t-test on the global-processing mean scores between the responsive and unresponsive conditions. A significant mean difference, such that the responsive condition shows a higher global processing score, would replicate the original finding. If a significant difference is found, I am also interested to see if the magnitude of the effect size is similar.

#### Additional analyses
Additional analyses will see if the effect persists after controlling for mood, as well as test for an interaction with participant sex, to see if the authors' other primary findings replicate.

For my own curiosity, I will also rerun all analyses without any data exclusions, as the original exclusions were substantial. 

Because the original dataset is public, I will also run a t-test between conditions in their data and compare my findings to that re-analysis (which more closely matches the statistical test I will be using).

### Differences from Original Study
1. In the original study, participants were not compensated for participating, whereas in this replication attempt, participants will be compensated monetarily. I do not expect this to make a difference in the effect of interest. While participants might be differently motivated, this should not bias the effect in a particular direction.

2. This replication attempt is underpowered to support a 3rd condition (the neutral control), so it will only examine the difference between the responsive and unresponsive conditions, rather than conducting an ANOVA between all three. This will make a difference in the computation of the effect, given that I will end up conducting a t-test (using the independent variance from two groups) rather than the authors' post-hoc Fisher's LSD test (which would use the pooled variance from all three). However, I will also conduct the same statistical test on the authors' original data and compare my results to that as well.

3. The authors included some additional questions^[These questions were: how often participants chose shapes based on constituent similarity and how often participants chose shapes based on pattern similarity.] in their survey materials. These items are not part of the specific effect I am planning to replicate, so I am excluding them from the survey for the sake of time. Because these measures were collected after both the manipulation and DV task, I do not expect them to make a difference in the effect of interest.

4. I decided to use a different attention check that is more straightforward and may be perceived less like we are trying to 'trick' or 'catch' participants. This change would only impact the effect of interest if mine or the authors' original attention check selectively excluded participants from one condition more than the other. I do not know if this was the case for the original study (in which case I would argue that the current attention check is a more valid test of the hypothesis), but I will confirm in my sample that roughly equal numbers of participants were excluded from each condition. 

### Methods Addendum (Post Data Collection)

#### Actual Sample

I recruited 212 English-speaking adults on Prolific. After exclusions, I was left with 208 participants for analysis. Consistent with Reis et al. (2018), the only demographic we collected was gender: Our sample composed of 40.9% male, 56.3% female, and 2.8% non-binary.


#### Differences from pre-data collection methods plan
Before launching the survey, the original authors advised me that they had randomized the global-local processing trials. I updated my reimplementation accordingly. Unfortunately, by adding randomization to each question of the task, Qualtrics eliminated the page breaks I had placed between trials. This means that all trials were presented on the same page. Additionally, since questions had been designed to auto-advance, after selecting their first answer participants got a warning message asking if they wanted to proceed to the next page despite skipping 15 questions, or stay on the page and answer the rest. We still received complete data from almost all participants, so it seems like most people made it through this step. However, the task may have been more confusing to participants than we intended for this reason.


## Results

### Data preparation
	
```{r, include=F}

####Load Relevant Libraries and Functions
library(here)

```

===Reproducing author results===

First, I will reproduce the authors' results from their public data, and I will run a t-test between the responsive and unresponsive conditions so that I have a directly comparable statistical test. I was able to reproduce the same means and test statistics as the original article. I also computed a t-test, resulting in a value of p = .0006, t(337.82) = 3.46.
```{r Reproducing author results, warning=FALSE}

####Load Relevant Libraries and Functions
library(tidyverse)
library(haven)

#Reproducing original author analysis
authordata <- read_sav(here("data/original_authordata/study5.sav"))

m <- aov(Globaltotal_total ~ Conditions, data = authordata)
summary(m) 

authordata %>% 
  group_by(condition) %>% 
  summarize(mean = mean(Globaltotal_total),
            sd = sd(Globaltotal_total)) 

#Determining test statistic if I conduct a t-test between  conditions of interest in the authors' original data
authordata_subset <- subset(authordata, Conditions != "Control")
t.test(Globaltotal_total ~ Conditions, authordata_subset)


```

===Importing new raw data===

Next I will turn to the analysis of my replication data.
```{r, Import raw data, warning=F}

####Import data
df_names <- read_csv(here("data/rawdata/rawdata.csv"), n_max=0) %>% names()

rawdata <- read_csv(here("data/rawdata/rawdata.csv"), 
                    col_names = df_names,
                    col_types = "d",
                    skip = 3)

```

===Cleaning the data===

There were only 4 exclusions, leaving a final sample of n = 208. There were an equal number of exclusions in each experimental condition.
```{r Data cleaning}
#### Data exclusion / filtering

skipped.morethan.8 <- function(row){
    ifelse(sum(is.na(row))>=8, 1,0)
}   

same.response.16 <- function(row){
  ifelse(sum(row)==16 | sum(row)==0, 1,0)
}

rrt.blank <- function(row){
  ifelse(is_empty(row)==TRUE, 1,0)
}

rawdata$exclude_skipped <- apply(select(rawdata, starts_with("trial_")),MARGIN=1, FUN=skipped.morethan.8)  

rawdata$exclude_sameresponse <- apply(select(rawdata, starts_with("trial_")),MARGIN=1, FUN=same.response.16)  

rawdata$exclude_rrt_blank <- apply(select(rawdata, starts_with("rrt_")),MARGIN=1, FUN=rrt.blank)

cleandata <- rawdata %>% 
  filter(Finished == 1 & attn_check_fruit == 1 & exclude_skipped == 0 & exclude_sameresponse==0 & exclude_rrt_blank==0)

dim(cleandata)

```

===Creating variables for analysis===
```{r Creating usable data}

# Converting some variables to factors
cleandata$condition <- as.factor(cleandata$condition)
cleandata$gender <- as.factor(cleandata$gender)
cleandata$prior_knowledge_rrt <- as.factor(cleandata$prior_knowledge_rrt)

#### Creating NA imputation function
na.mean <- function(row){
    replace(row,is.na(row)==T,mean(row, na.rm=T))
}

#### Imputing NA values for small and large trials

cleandata[,c(names(select(cleandata, starts_with("trial_sm"))))] <- sapply(cleandata[,c(names(select(cleandata, starts_with("trial_sm"))))], na.mean)

cleandata[,c(names(select(cleandata, starts_with("trial_lg"))))] <- sapply(cleandata[,c(names(select(cleandata, starts_with("trial_lg"))))], na.mean)

#### Prepare data for analysis - create columns etc.
data <- cleandata %>% 
  mutate(global_small = rowSums(select(cleandata, starts_with("trial_sm")), na.rm = TRUE)) %>% 
  mutate(global_large = rowSums(select(cleandata, starts_with("trial_lg")), na.rm = TRUE)) %>% 
  mutate(global_total = global_small + global_large) %>% 
  mutate(positive = rowMeans(cleandata[,c("panas_1", "panas_3", "panas_5", "panas_9", "panas_10", "panas_12", "panas_14", "panas_16", "panas_17", "panas_19")])) %>% 
  mutate(negative = rowMeans(cleandata[,c("panas_2", "panas_4", "panas_6", "panas_7", "panas_8", "panas_11", "panas_13", "panas_15", "panas_18", "panas_20")])) %>% 
  mutate(mood = positive - negative) %>% 
  unite("partner", c(partner_type_u, partner_type_r), na.rm=TRUE) %>%
  mutate(partner = as.factor(partner)) %>% 
  unite("difficulty", c(difficulty_u, difficulty_r), na.rm=TRUE) %>%
  mutate(difficulty = as.numeric(difficulty)) %>% 
  select(c("condition", "global_small", "global_large", "global_total", "mood", "gender", "partner", "prior_knowledge_rrt", "difficulty", "device"))

```


=== Condition, manipulation, and gender checks ===

First, I'll look at how many people ended up in each condition. Although the experiment was designed to randomize people equally into each group, more participants ended up in the responsive condition (110) than the unresponsive condition (98), probably due people dropping out of the unresponsive condition after starting the survey.

```{r}

table(data$condition)

```

Next I'll run a manipulation check to see if the relationship recall task was indeed harder for participants in the unresponsive condition. According to the test below, the unresponsive writing condition was indeed more challenging Mdiff = .61 (on a 1-5 scale), p = .0003.

```{r}

t.test(difficulty ~ condition, data)


```

Checking gender: according to the table below, there were more females randomized into the responsive condition vs. the unresponsive condition.
```{r}
data$gender <- ifelse(data$gender==1, "Male", ifelse(data$gender == 2, "Female", "Non-binary"))
table(data$condition, data$gender)

```

### Confirmatory analysis

#### Primary test
Now I will move to the single confirmatory test, a t-test on the global processing scores between conditions:
```{r}

t.test(global_total ~ condition, data)

```

Recreating the results table for comparison:
```{r}

table <- data %>% 
  group_by(condition) %>% 
  summarize(global = round(mean(global_total),2), sd = round(sd(global_total, na.rm=T),2), n = n()) %>% 
  t() %>% 
  as.data.frame()

table
  

```

Compare to original table:
<center>
![Table 6, Reis et al. (2018), p. 30](/Users/catherinegarton/Documents/Grad School/Class and admin/251/reis2018/images/authortable.png){width=50%}
</center>

Visualizing the results:
```{r}

plotdata <- data %>% 
  group_by(condition) %>%
  dplyr::summarize(avgscore = mean(global_total), sd = sd(global_total, na.rm = T), n =n()) %>% 
  mutate(upper = (avgscore + (sd/sqrt(n))*1.96), lower = (avgscore - (sd/sqrt(n))*1.96))


ggplot(plotdata, aes(x=condition, y=avgscore)) +
  geom_col(fill="gray") +
  geom_errorbar(aes(ymin=lower,ymax=upper), width = .05) +
  labs(y = "Average global processing score", x = "", title = "Global orientation by condition") +
  coord_cartesian(ylim = c(0,16)) +
  scale_x_discrete(labels=c("Responsive", "Unresponsive")) +
  theme_bw() + 
  theme(plot.title = element_text(hjust=.5))


```


### Exploratory analyses

```{r }

summary(lm(global_total ~ condition, data)) # basic model, no controls
summary(lm(global_total ~ condition + mood, data)) # controlling for mood
summary(lm(global_total ~ condition*gender, data)) # testing for interaction with gener
summary(lm(global_total ~ condition*partner, data)) # testing for interaction with partner category (friend vs. SO)
summary(lm(global_total ~ device, data)) # testing whether device matters

```
None of these models show significance. This suggests that--at least in my sample--there was no effect of condition, mood, gender, partner type, or device type on global-processing scores.

## Discussion

### Summary of Replication Attempt

The original finding was that participants in the responsive condition had significantly higher global processing scores than participants in the unresponsive condition.

This primary statistical test did not replicate. I ran a t-test between the unresponsive and responsive conditions, and my findings showed the reverse trend than the original authors found. The **unresponsive** condition had a higher mean global processing score (10.3) in comparison to the responsive condition (9.4). Moreover, the difference bewteen these groups was not actually statistically significant.

### Commentary

It is unclear to me why this result did not replicate, but I can think of one possible idea. There was a single difference between the original study and my replication, which is that I opted to use a different attention check. In Reis et al. (2018), 30% of the sample was excluded for failing the attention check. The authors' attention check required participants to carefully read a paragraph-long block of text, and then respond to a deceptive question. In order to get this question right, participants had to be detail-oriented and read the whole paragraph in order to spot the attention check. This is a direct confound to the experimental task, which tests whether people focused on big-picture patterns or the details of the constituent parts. 

Although I cannot know for sure, my guess is that people with higher global-processing scores were disproportionately excluded from the original study. If there was unhappy randomization (in terms of participants' dispositional tendencies to process globally vs. locally), this may have accidentally deflated the unresponsive condition.

Alternatively, my reimplementation had a glitch, which caused all of the trials to display on the same page. I don't think this would bias one condition over another, but it does make my replication less neat and adds potential random noise.

As for author input, Prof. Reis was extremely helpful in reviewing my materials and providing feedback. At his suggestion, I randomized the figure trials (to be more consistent with the original experiment) and eliminated an extra question I had added that wasn't in the original. Besides those notes, Prof. Reis did not object to the rest of our reimplementation decisions. He did note that we had dropped the third condition and were using a different attention check, but he accepted the rationale for both. 
